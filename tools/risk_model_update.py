# åˆ›ä¸šå…¬å¸é£é™©é¢„æµ‹æ¨¡å‹ - æ”¹è¿›ç‰ˆæœ¬
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
import shap

warnings.filterwarnings('ignore')

class StartupRiskPredictor:
    def __init__(self, data_path):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        """
        self.data_path = data_path
        self.df_raw = None
        self.df_final = None
        self.models = {}
        self.feature_names = None
        
    def load_and_explore_data(self):
        """
        1. æ•°æ®åŠ è½½å’Œæ¢ç´¢
        """
        print("=" * 50)
        print("ğŸ“Š æ•°æ®åŠ è½½å’Œæ¢ç´¢")
        print("=" * 50)
        
        try:
            self.df_raw = pd.read_csv(self.data_path)
            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼Œå½¢çŠ¶: {self.df_raw.shape}")
            
            # ç¼ºå¤±å€¼ç»Ÿè®¡
            missing = self.df_raw.isnull().sum()
            missing_stats = missing[missing > 0].sort_values(ascending=False)
            
            if len(missing_stats) > 0:
                print("\nğŸ“‹ ç¼ºå¤±å€¼ç»Ÿè®¡:")
                for col, count in missing_stats.items():
                    pct = count / len(self.df_raw) * 100
                    print(f"  {col}: {count} ({pct:.1f}%)")
            else:
                print("âœ… æ— ç¼ºå¤±å€¼")
                
            return self.df_raw.head()
            
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {self.data_path}")
            return None
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def clean_data(self):
        """
        2. æ•°æ®æ¸…æ´—
        """
        print("\n" + "=" * 50)
        print("ğŸ§¹ æ•°æ®æ¸…æ´—")
        print("=" * 50)
        
        if self.df_raw is None:
            print("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return None
            
        # åˆ é™¤å…³é”®å­—æ®µç¼ºå¤±çš„è¡Œ
        critical_columns = ['founded_at', 'category_list', 'country_code']
        initial_rows = len(self.df_raw)
        
        df_clean = self.df_raw.dropna(subset=critical_columns)
        
        print(f"åŸå§‹è¡Œæ•°: {initial_rows}")
        print(f"æ¸…æ´—åè¡Œæ•°: {len(df_clean)} (ä¿ç•™ {len(df_clean)/initial_rows*100:.1f}%)")
        
        # ç›®æ ‡å˜é‡åˆ†æ
        if 'status' in df_clean.columns:
            status_counts = df_clean['status'].value_counts(dropna=False)
            print(f"\nğŸ“ˆ å…¬å¸çŠ¶æ€åˆ†å¸ƒ:")
            for status, count in status_counts.items():
                pct = count / len(df_clean) * 100
                print(f"  {status}: {count} ({pct:.1f}%)")
        
        self.df_clean = df_clean
        return df_clean
    
    def feature_engineering(self):
        """
        3. ç‰¹å¾å·¥ç¨‹
        """
        print("\n" + "=" * 50)
        print("ğŸ”§ ç‰¹å¾å·¥ç¨‹")
        print("=" * 50)
        
        if not hasattr(self, 'df_clean'):
            print("âŒ è¯·å…ˆè¿›è¡Œæ•°æ®æ¸…æ´—")
            return None
            
        df_model = self.df_clean.copy()
        
        # åˆ›å»ºç›®æ ‡å˜é‡
        df_model['is_risky'] = (df_model['status'] == 'closed').astype(int)
        print(f"é£é™©æ ·æœ¬æ¯”ä¾‹: {df_model['is_risky'].mean():.3f}")
        
        # æå–æˆç«‹å¹´ä»½
        df_model['founded_year'] = pd.to_datetime(
            df_model['founded_at'], 
            errors='coerce'
        ).dt.year
        
        # æ¸…ç†èèµ„é‡‘é¢
        if 'funding_total_usd' in df_model.columns:
            df_model['funding_total_usd'] = (
                df_model['funding_total_usd']
                .astype(str)
                .str.replace(r'[\$,]', '', regex=True)
                .replace(['', '-', 'nan'], np.nan)
                .astype(float)
            )
            
            # ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼
            median_funding = df_model['funding_total_usd'].median()
            df_model['funding_total_usd'] = df_model['funding_total_usd'].fillna(median_funding)
            print(f"èèµ„é‡‘é¢ä¸­ä½æ•°å¡«å……: ${median_funding:,.0f}")
        
        # å¤„ç†åˆ†ç±»ç‰¹å¾
        # ç®€åŒ–è¡Œä¸šç±»åˆ«
        df_model['main_category'] = df_model['category_list'].apply(
            lambda x: str(x).split('|')[0] if '|' in str(x) else str(x)
        )
        
        # ç®€åŒ–å›½å®¶
        top_countries = df_model['country_code'].value_counts().head(10).index
        df_model['country_grouped'] = df_model['country_code'].apply(
            lambda x: x if x in top_countries else 'Other'
        )
        
        # One-Hotç¼–ç 
        categorical_features = ['country_grouped', 'main_category']
        df_encoded = pd.get_dummies(
            df_model[categorical_features], 
            prefix=['country', 'category']
        )
        
        # é€‰æ‹©æ•°å€¼ç‰¹å¾
        numeric_features = ['founded_year', 'funding_total_usd', 'funding_rounds']
        available_numeric = [col for col in numeric_features if col in df_model.columns]
        
        # åˆå¹¶ç‰¹å¾
        self.df_final = pd.concat([
            df_model[available_numeric + ['is_risky']],
            df_encoded
        ], axis=1)
        
        # å¤„ç†å‰©ä½™ç¼ºå¤±å€¼
        numeric_cols = self.df_final.select_dtypes(include=[np.number]).columns
        numeric_cols = numeric_cols.drop('is_risky')
        
        for col in numeric_cols:
            if self.df_final[col].isnull().any():
                fill_value = self.df_final[col].median() if col != 'founded_year' else 2010
                self.df_final[col] = self.df_final[col].fillna(fill_value)
                print(f"å¡«å…… {col} ç¼ºå¤±å€¼: {fill_value}")
        
        print(f"âœ… æœ€ç»ˆç‰¹å¾ç»´åº¦: {self.df_final.shape}")
        print(f"âœ… æ— ç¼ºå¤±å€¼: {not self.df_final.isnull().any().any()}")
        
        return self.df_final
    
    def prepare_train_test_split(self, test_size=0.2, random_state=42):
        """
        4. è®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†
        """
        from sklearn.model_selection import train_test_split
        
        if self.df_final is None:
            print("âŒ è¯·å…ˆè¿›è¡Œç‰¹å¾å·¥ç¨‹")
            return None
            
        X = self.df_final.drop("is_risky", axis=1)
        y = self.df_final["is_risky"]
        
        self.feature_names = X.columns.tolist()
        
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )
        
        print(f"\nğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
        print(f"è®­ç»ƒé›†: {len(self.X_train)} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(self.X_test)} æ ·æœ¬")
        print(f"è®­ç»ƒé›†é£é™©æ¯”ä¾‹: {self.y_train.mean():.3f}")
        print(f"æµ‹è¯•é›†é£é™©æ¯”ä¾‹: {self.y_test.mean():.3f}")
        
    def train_models(self):
        """
        5. æ¨¡å‹è®­ç»ƒ - ä½¿ç”¨å¤šç§ç®—æ³•
        """
        print("\n" + "=" * 50)
        print("ğŸ¤– æ¨¡å‹è®­ç»ƒ")
        print("=" * 50)

        from sklearn.linear_model import LogisticRegression
        from sklearn.ensemble import RandomForestClassifier, VotingClassifier
        from sklearn.metrics import classification_report, roc_auc_score
        import joblib  # âœ… åŠ è½½ joblib

        try:
            from xgboost import XGBClassifier
            xgb_available = True
        except ImportError:
            print("âš ï¸ XGBoostæœªå®‰è£…ï¼Œå°†è·³è¿‡XGBæ¨¡å‹")
            xgb_available = False

        models = {
            'LogisticRegression': LogisticRegression(
                max_iter=1000, 
                class_weight='balanced', 
                random_state=42
            ),
            'RandomForest': RandomForestClassifier(
                n_estimators=100, 
                class_weight='balanced', 
                random_state=42
            )
        }

        if xgb_available:
            models['XGBoost'] = XGBClassifier(
                use_label_encoder=False,
                eval_metric='logloss',
                scale_pos_weight=10,
                random_state=42
            )

        self.models = {}
        model_scores = {}

        for name, model in models.items():
            print(f"\nğŸ”„ è®­ç»ƒ {name}...")
            try:
                model.fit(self.X_train, self.y_train)
                y_pred = model.predict(self.X_test)
                y_proba = model.predict_proba(self.X_test)[:, 1]

                auc_score = roc_auc_score(self.y_test, y_proba)
                model_scores[name] = auc_score
                self.models[name] = model

                print(f"âœ… {name} AUC: {auc_score:.3f}")

            except Exception as e:
                print(f"âŒ {name} è®­ç»ƒå¤±è´¥: {str(e)}")

        if len(self.models) > 1:
            print(f"\nğŸ”„ è®­ç»ƒé›†æˆæ¨¡å‹...")
            ensemble_models = [(name, model) for name, model in self.models.items()]
            voting_clf = VotingClassifier(
                estimators=ensemble_models,
                voting='soft'
            )

            voting_clf.fit(self.X_train, self.y_train)
            y_proba_ensemble = voting_clf.predict_proba(self.X_test)[:, 1]
            auc_ensemble = roc_auc_score(self.y_test, y_proba_ensemble)

            self.models['Ensemble'] = voting_clf
            model_scores['Ensemble'] = auc_ensemble
            print(f"âœ… Ensemble AUC: {auc_ensemble:.3f}")

        best_model_name = max(model_scores, key=model_scores.get)
        self.best_model = self.models[best_model_name]
        self.best_model_name = best_model_name

        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name} (AUC: {model_scores[best_model_name]:.3f})")

        # âœ… ä¿å­˜æ¨¡å‹ä¸ç‰¹å¾åˆ—å
        joblib.dump(self.best_model, "risk_model.joblib")
        joblib.dump(self.feature_names, "model_feature_names.joblib")
        print("âœ… å·²ä¿å­˜æ¨¡å‹åˆ° risk_model.joblib")
        print("âœ… å·²ä¿å­˜ç‰¹å¾åˆ—åˆ° model_feature_names.joblib")

        # SHAP åˆ†æå™¨ï¼ˆä»…é™ XGBoostï¼‰
        if self.best_model_name == 'XGBoost':
            try:
                X_train_float = self.X_train.astype(np.float64)
                X_test_float = self.X_test.astype(np.float64)
                explainer = shap.Explainer(self.best_model, X_train_float)
                self.shap_values = explainer(X_test_float)
            except Exception as e:
                print(f"âŒ SHAP åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                self.shap_values = None
        else:
            print("âš ï¸ å½“å‰æœ€ä½³æ¨¡å‹ä¸æ˜¯ XGBoostï¼Œè·³è¿‡ SHAP åˆ†æ")
            self.shap_values = None

        return self.models
    
    def evaluate_model(self, model_name=None, custom_threshold=0.5):
        """
        6. æ¨¡å‹è¯„ä¼°
        """
        print("\n" + "=" * 50)
        print("ğŸ“ˆ æ¨¡å‹è¯„ä¼°")
        print("=" * 50)
        
        from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
        
        if model_name is None:
            model = self.best_model
            model_name = self.best_model_name
        else:
            model = self.models.get(model_name)
            if model is None:
                print(f"âŒ æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
                return None
        
        # é¢„æµ‹
        y_proba = model.predict_proba(self.X_test)[:, 1]
        y_pred = (y_proba >= custom_threshold).astype(int)
        
        # è¯„ä¼°æŒ‡æ ‡
        auc_score = roc_auc_score(self.y_test, y_proba)
        
        print(f"æ¨¡å‹: {model_name}")
        print(f"é˜ˆå€¼: {custom_threshold}")
        print(f"AUC Score: {auc_score:.3f}")
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(self.y_test, y_pred))
        print("\næ··æ·†çŸ©é˜µ:")
        print(confusion_matrix(self.y_test, y_pred))
        
        return {
            'auc_score': auc_score,
            'y_proba': y_proba,
            'y_pred': y_pred
        }
    
    def plot_threshold_analysis(self):
        """
        7. é˜ˆå€¼åˆ†æå¯è§†åŒ–
        """
        from sklearn.metrics import roc_auc_score
        from sklearn.metrics import precision_recall_curve
        
        if not hasattr(self, 'best_model'):
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return
            
        y_proba = self.best_model.predict_proba(self.X_test)[:, 1]
        precision, recall, thresholds = precision_recall_curve(self.y_test, y_proba)
        f1 = 2 * (precision * recall) / (precision + recall + 1e-8)
        
        plt.figure(figsize=(12, 8))
        
        plt.subplot(2, 2, 1)
        plt.plot(thresholds, precision[:-1], label="Precision", linewidth=2)
        plt.plot(thresholds, recall[:-1], label="Recall", linewidth=2)
        plt.plot(thresholds, f1[:-1], label="F1 Score", linewidth=2)
        plt.axvline(x=0.5, color='gray', linestyle='--', alpha=0.7, label='Default (0.5)')
        plt.xlabel("Threshold")
        plt.ylabel("Score")
        plt.title("Precision / Recall / F1 vs Threshold")
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ROCæ›²çº¿
        from sklearn.metrics import roc_curve
        fpr, tpr, _ = roc_curve(self.y_test, y_proba)
        auc = roc_auc_score(self.y_test, y_proba)
        
        plt.subplot(2, 2, 2)
        plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc:.3f})')
        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        plt.subplot(2, 2, 3)
        plt.hist(y_proba[self.y_test == 0], bins=50, alpha=0.7, label='Non-risky', density=True)
        plt.hist(y_proba[self.y_test == 1], bins=50, alpha=0.7, label='Risky', density=True)
        plt.xlabel('Predicted Probability')
        plt.ylabel('Density')
        plt.title('Prediction Probability Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ç‰¹å¾é‡è¦æ€§ (å¦‚æœæ˜¯æ ‘æ¨¡å‹)
        plt.subplot(2, 2, 4)
        if hasattr(self.best_model, 'feature_importances_'):
            importances = self.best_model.feature_importances_
            indices = np.argsort(importances)[-15:]  # top 15
            plt.barh(range(len(indices)), importances[indices])
            plt.yticks(range(len(indices)), [self.feature_names[i] for i in indices])
            plt.xlabel('Feature Importance')
            plt.title('Top 15 Feature Importances')
        else:
            plt.text(0.5, 0.5, 'Feature importance\nnot available\nfor this model', 
                    ha='center', va='center', transform=plt.gca().transAxes)
            plt.title('Feature Importance')
        
        plt.tight_layout()
        plt.show()
    
    def get_feature_importance(self, top_n=20):
        """
        8. è·å–ç‰¹å¾é‡è¦æ€§
        """
        if not hasattr(self, 'best_model'):
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
            return None
            
        model = self.best_model
        
        # ä¸åŒæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§è·å–æ–¹å¼
        if hasattr(model, 'feature_importances_'):
            # æ ‘æ¨¡å‹
            importances = model.feature_importances_
        elif hasattr(model, 'coef_'):
            # çº¿æ€§æ¨¡å‹
            importances = np.abs(model.coef_[0])
        elif hasattr(model, 'estimators_'):
            # é›†æˆæ¨¡å‹ - å°è¯•è·å–å¹³å‡é‡è¦æ€§
            try:
                all_importances = []
                for name, estimator in model.named_estimators_.items():
                    if hasattr(estimator, 'feature_importances_'):
                        all_importances.append(estimator.feature_importances_)
                    elif hasattr(estimator, 'coef_'):
                        all_importances.append(np.abs(estimator.coef_[0]))
                
                if all_importances:
                    importances = np.mean(all_importances, axis=0)
                else:
                    print("âš ï¸ æ— æ³•è·å–é›†æˆæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§")
                    return None
            except:
                print("âš ï¸ æ— æ³•è·å–é›†æˆæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§")
                return None
        else:
            print("âš ï¸ å½“å‰æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
            return None
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        feature_importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False).head(top_n)
        
        print(f"\nğŸ” Top {top_n} é‡è¦ç‰¹å¾ ({self.best_model_name}):")
        print(feature_importance_df.to_string(index=False))
        
        return feature_importance_df
    
    def generate_shap_summary(self, top_n=20):
        """
        9. ç”Ÿæˆ SHAP æ€»ç»“å›¾
        """
        print("\n" + "=" * 50)
        print("ğŸ” SHAP è§£é‡Šåˆ†æ")
        print("=" * 50)

        if not hasattr(self, 'shap_values') or self.shap_values is None:
            print("âŒ SHAP è§£é‡Šå°šæœªç”Ÿæˆæˆ–ä¸é€‚ç”¨äºå½“å‰æ¨¡å‹")
            return

        # ç”» SHAP ç‰¹å¾é‡è¦æ€§å›¾ï¼ˆç±»ä¼¼æ¡å½¢å›¾ï¼‰
        shap.summary_plot(self.shap_values, self.X_test, plot_type='bar', max_display=top_n)

        # ç”» SHAP åˆ†å¸ƒå›¾ï¼ˆæ¯ä¸ªç‰¹å¾å¦‚ä½•å½±å“é¢„æµ‹ï¼‰
        shap.summary_plot(self.shap_values, self.X_test, max_display=top_n)

    def predict_and_explain(self, company_dict):
        """
        è¾“å…¥å…¬å¸ä¿¡æ¯ï¼Œè¾“å‡ºé£é™©é¢„æµ‹ç»“æœï¼Œå¹¶ç”¨ SHAP force plot å¯è§†åŒ–è§£é‡Š
        """
        import pandas as pd
        import numpy as np
        import shap
        import matplotlib.pyplot as plt

        if self.best_model is None or self.shap_values is None:
            print("âŒ è¯·å…ˆè®­ç»ƒæ¨¡å‹å¹¶å®Œæˆ SHAP åˆå§‹åŒ–")
            return

        # 1. æ„é€ è¾“å…¥æ•°æ®ï¼ˆå¤ç”¨å‰é¢å‡½æ•°é€»è¾‘ï¼‰
        new_df = pd.DataFrame([company_dict])
        new_df['is_risky'] = 0  # å ä½
        new_df['founded_year'] = pd.to_datetime(new_df['founded_at'], errors='coerce').dt.year

        if 'funding_total_usd' in new_df.columns:
            new_df['funding_total_usd'] = (
                new_df['funding_total_usd']
                .astype(str)
                .str.replace(r'[\$,]', '', regex=True)
                .replace(['', '-', 'nan'], np.nan)
                .astype(float)
            )
            if np.isnan(new_df['funding_total_usd'].iloc[0]):
                new_df['funding_total_usd'] = self.df_clean['funding_total_usd'].median()

        new_df['main_category'] = new_df['category_list'].apply(
            lambda x: str(x).split('|')[0] if '|' in str(x) else str(x)
        )
        top_countries = self.df_clean['country_code'].value_counts().head(10).index
        new_df['country_grouped'] = new_df['country_code'].apply(
            lambda x: x if x in top_countries else 'Other'
        )

        # One-hot ç¼–ç 
        categorical_features = ['country_grouped', 'main_category']
        encoded = pd.get_dummies(new_df[categorical_features], prefix=['country', 'category'])

        for col in self.df_final.columns:
            if col.startswith("country_") or col.startswith("category_"):
                if col not in encoded.columns:
                    encoded[col] = 0

        numeric_features = ['founded_year', 'funding_total_usd', 'funding_rounds']
        available_numeric = [col for col in numeric_features if col in new_df.columns]

        final_input = pd.concat([
            new_df[available_numeric],
            encoded
        ], axis=1)

        final_input = final_input.reindex(columns=self.feature_names, fill_value=0)
        final_input = final_input.astype(np.float64)

        # 2. åšé¢„æµ‹
        prob = self.best_model.predict_proba(final_input)[0][1]
        label = int(prob >= 0.5)

        print(f"\nğŸ§ª é¢„æµ‹ç»“æœ: {'é«˜é£é™© ğŸš¨' if label else 'ä½é£é™© âœ…'}ï¼Œæ¦‚ç‡ = {prob:.3f}")

        # 3. ç”Ÿæˆ SHAP force plotï¼ˆè§£é‡Šä¸ºä»€ä¹ˆï¼‰
        explainer = shap.Explainer(self.best_model, self.X_train.astype(np.float64))
        shap_value = explainer(final_input)

        shap.plots.force(shap_value[0], matplotlib=True)
        plt.title("SHAP Force Plot - é£é™©é¢„æµ‹è§£é‡Š")
        plt.show()

        return {'label': label, 'probability': prob}



def main():
    """
    ä¸»å‡½æ•° - å®Œæ•´çš„å»ºæ¨¡æµç¨‹
    """
    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = StartupRiskPredictor("../dataset/big_startup_secsees_dataset.csv")
    
    # æ‰§è¡Œå®Œæ•´æµç¨‹
    try:
        # 1. åŠ è½½æ•°æ®
        predictor.load_and_explore_data()
        
        # 2. æ¸…æ´—æ•°æ®
        predictor.clean_data()
        
        # 3. ç‰¹å¾å·¥ç¨‹
        predictor.feature_engineering()
        
        # 4. åˆ’åˆ†æ•°æ®é›†
        predictor.prepare_train_test_split()
        
        # 5. è®­ç»ƒæ¨¡å‹
        predictor.train_models()
        
        # 6. è¯„ä¼°æ¨¡å‹
        predictor.evaluate_model()
        
        # 7. è‡ªå®šä¹‰é˜ˆå€¼è¯„ä¼°
        print("\n" + "="*50)
        print("ğŸ¯ è‡ªå®šä¹‰é˜ˆå€¼åˆ†æ")
        print("="*50)
        
        for threshold in [0.3, 0.4, 0.5]:
            print(f"\n--- é˜ˆå€¼ {threshold} ---")
            predictor.evaluate_model(custom_threshold=threshold)
        
        # 8. å¯è§†åŒ–åˆ†æ
        predictor.plot_threshold_analysis()
        
        # 9. ç‰¹å¾é‡è¦æ€§
        predictor.get_feature_importance()

        # 10.shapåˆ†æ
        predictor.generate_shap_summary()

        test_company_a = {
            'founded_at': '2010-05-20',                      # æˆç«‹è¾ƒæ—©
            'category_list': 'Biotechnology|Healthcare',     # ç”Ÿç‰©åŒ»è¯ç±»ï¼ˆæ¨¡å‹ä¸­è¡¨ç°è¾ƒå¥½ï¼‰
            'country_code': 'USA',
            'funding_total_usd': '20000000',                 # èèµ„é‡‘é¢å¤§
            'funding_rounds': 5                              # èèµ„è½®æ•°å¤š
        }

        test_company_b = {
            'founded_at': '2018-11-01',                      # æˆç«‹æ—¶é—´è¾ƒè¿‘
            'category_list': 'Games|Curated Web',            # å¨±ä¹ç±»ï¼ˆæ¨¡å‹ä¸­é£é™©é«˜ï¼‰
            'country_code': 'RUS',                           # ä¸åœ¨ Top10 å›½å®¶ï¼Œä¼šè¢«å½’ä¸º "Other"
            'funding_total_usd': '30000',                    # èèµ„é‡‘é¢æä½
            'funding_rounds': 0                              # æ— èèµ„å†å²
        }


        print("ğŸ¯ æµ‹è¯•ç”¨ä¾‹ Aï¼ˆä½é£é™©å…¬å¸ï¼‰")
        predictor.predict_and_explain(test_company_a)

        print("\nğŸ¯ æµ‹è¯•ç”¨ä¾‹ Bï¼ˆé«˜é£é™©å…¬å¸ï¼‰")
        predictor.predict_and_explain(test_company_b)
        
        print("\nâœ… å»ºæ¨¡æµç¨‹å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ å»ºæ¨¡è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()